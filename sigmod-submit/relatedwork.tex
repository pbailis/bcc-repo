
\section{Related Work}
\label{sec:relatedwork}

The research literature has a long tradition of using semantic
information in concurrency control for improved performance,
scalability, and availability.

% integrity constraints predate serializability; references here for
% substantial work on rewriting, maintaining, and minimizing
% computation cost for given integrity constraints-- our focus here is
% on semantics that can be achieved without coordination. our focus
% here is on replicated, non-atomic transactions.

\minihead{Integrity constraints} Use of database integrity constraints
dates to at least 1974~\cite{florentin-constraints} and has been
studied extensively (see \cite{tamer-book} for an
summary). As~\cite{ic-survey,ic-survey-two} survey, a large body of
work examines how to perform query rewriting, transaction analysis,
and database design to accommodate a range of integrity
constraints. As \"{O}zsu and Valduriez~\cite{tamer-book} discuss, this
work largely presumes single-node databases (i.e., atomic---and
therefore non-\cfree---updates to shared state) and/or the use of
global concurrency control (for both prevention- and detection-based
approaches). Notably,~\cite{local-verification} avoids global
concurrency control and studies the problem of verifying constraints
in a shared-nothing, partitioned (but non-replicated) database system,
while~\cite{kemme-si-ic} discusses the maintenance of common integrity
constraints under replicated (non-\cfree~\cite{hat-vldb}) Snapshot
Isolation. Our goal to determine when we can avoid global concurrency
control and any coordination between replicas.

% following serializability, looked at semantics for concurrency
% control

\minihead{Semantics-based Concurrency Control} A related body of
research uses semantic information to re-define correctness criteria
for shared databases. \"{O}zsu and Valduriez~\cite{tamer-book} provide
a brief summary of this work, which, again, largely focuses on global
(i.e., atomic, serializable, or single-site) concurrency control
strategies, but we discuss several notable approaches here.

Much of semantics-based concurrency control uses application semantics
as a means to reduce conflicts during validation or execution of
concrete schedules of transactions (at
runtime)~\cite{badrinath-semantics} (i.e., via commutativity
analysis~\cite{weihl-thesis} or serial dependency
relations~\cite{herlihy-apologizing}). This is eminently useful when,
indeed, conflicts are possible. However, this validation (and conflict
detection) requires communication between processes to reach commit
decisions. We seek to identify semantics that are achievable entirely
without coordination: \iconfluence analysis statically reason about
all \textit{possible} schedules of transactions instead of performing
run-time validation.

SDD-1~\cite{sdd1} and Garcia-Molina~\cite{garciamolina-semantics}'s
compatibility sets describe (manually-labeled) transaction classes
that can be safely interleaved as a series of atomic steps (producing
``semantically consistent schedules'' similar to Korth's
predicate-wise serializability~\cite{korth-serializability}). Our
\iconfluence reasons about divergent (non-atomic) executions on
multiple replicas but could be used to produce these compatibility
sets. Assertional Concurrency Control~\cite{decomp-semantics}
decomposes atomic transactions (like chopping~\cite{chopping} and
nested atomic transactions~\cite{atomictransactions}) by requiring
Hoare-style pre- and post-conditions for each individual operation and
performing axiomatic program analysis (and has also been applied to
single-site isolation models~\cite{isolation-semantics}). We use a
single, database-wide set of invariants, which obviates the need for
manually labeling transaction types. A range of extended transaction
models~\cite{acta} can further reduce conflicts once it is established
that invariant-based conflicts can actually occur.

\minihead{State-based Commutativity} Related work often reasons about
the commutativity of transaction \textit{outcomes}~\cite{boosting}:
for example, two transactions provide state-based commutativity if
their return value is the same the the final state of the database is
equivalent despite reordering~\cite{weihl-data,weihl-thesis}. This
state-based commutativity is a sufficient but not necessary condition
for concurrent execution. Despite its conservativity, these techniques
have been successfully applied in diverse fields including database
concurrency control, concurrent programming~\cite{boosting}, recently,
operating systems design~\cite{kohler-commutativity}. State-based
commutativity analysis does not require the specification of
application-level invariants but, as~\cite{kohler-commutativity}
notes, is not necessary for maintaining correctness for all
applications~\cite{lamport-audit}.

\minihead{Term rewriting} Our use of \iconfluence is inspired by the
literature on term rewriting and constraint programming. An
\iconfluent rewrite system guarantees that arbitrary rule application
will not violate a given invariant~\cite{obs-confluence}, generalizing
Church-Rosser confluence~\cite{termrewriting}. We effectively treat
transactions as rewrite rules, database states as constraint states,
and the database merge operator as a special \textit{join} operator
defined for all database states.  Rewriting system
concepts---including confluence~\cite{aiken-confluence}---have been
successfully integrated into active database
systems~\cite{activedb-book} (e.g., triggers and rule processing),
although we are not familiar with a concept analogous to \iconfluence
in this literature. We see rewriting systems as an interesting
starting point for future theoretical analysis.

\minihead{Program analysis} Maintaining correctness despite concurrent
access is well studied in the programming languages
community~\cite{schneider-correctness}. In particular, \iconfluence
condition is closely related to Owicki-Gries interference
freedom~\cite{owickigries}, whereby concurrent operations cannot
interfere with one another's preconditions for execution, as well as
Lamport's monotone
assertions~\cite{lamport-safety}. As~\cite{decomp-semantics}
and~\cite{agarwal-consistency} demonstrate, much of this theory for
axomatic decomposition of concurrent programs is applicable to
analysis of transaction schedules. However, this literature (yet
again) almost exclusively considers atomic update to shared state (as
is reasonable on a multiprocessor system), so the techniques are not
immediately portable to a model with replicated, diverging state as we
consider here.

\minihead{Hoping and Apologizing} In this work, we have assumed that
database state should \textit{always} be consistent with respect to
invariants. Some applications can instead benefit from
probabilistically or numerically-bounded deviations from consistent
state~\cite{epsilon-divergence} or can provide compensating
transactions to account for concurrent behavior (e.g.,
Sagas~\cite{sagas})~\cite{ic-survey,ic-survey-two}. These strategies
require that programmers reason about inconsistent state or otherwise
write compensatory code.

\minihead{Liveness and Convergence} The CALM
Theorem~\cite{ameloot-calm} states that monotonic logic provides
confluent (deterministic) program outcomes despite message
re-ordering. Subsequent analyses in the Bloom~\cite{calm}, and
Bloom\textsuperscript{L}~\cite{blooml} languages and the
Blazes~\cite{blazes} system detect non-monotonic operations. This is a
useful \textit{liveness} guarantee---something good will
happen~\cite{lamport-safety}---but does not prevent users from
observing inconsistent database state---\textit{safety}---both during
execution and post-convergence. Here, we consider safety (in the form
of application-level integrity constraints) and also allow
non-deterministic (but safe) outcomes. CRDT objects~\cite{crdt}
similarly ensure convergent outcomes that reflect all updates made to
each object. This is useful in appropriately merging divergent
replicas on a per-item basis (and managing replicated state without
suffering from many forms of Lost Update) but does not guarantee
application-level correctness.

\minihead{High Availability and Scalability} A large class of systems
seeks to provide availability~\cite{gilbert-cap} via ``optimistic
replication''~\cite{optimistic}, which is complementary to our goal of
coordination-freedom. Red-Blue Consistency~\cite{redblue} specifically
examines mixed eventually consistent and linearizable models within a
single store; we seek an understanding of \textit{when} coordination
is necessary rather than an optimal implementation of a given
model. \cite{hat-vldb} recently classified isolation models according
to their availability but did not consider conditions for achieving
application-level consistency and instead focused on low-level
read/write isolation anomalies.  Johnson et al. have examined the
communication patterns of transactions~\cite{shore-communication}; we
focus on all-or-nothing communication requirements, but their
observations are useful for non-\iconfluent applications. A range of
mechanisms provide execution for non-\cfree
operations~\cite{silo,spanner,mdcc,bernstein-book,gray-book,hstore,megastore}.

