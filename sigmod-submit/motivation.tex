
\section{Coordination and Consistency}
\label{sec:motivation}

% application-level consistency is key requirement

As repositories for application state, databases are tasked with the
challenging goal of managing data despite concurrency, failures, and,
often, distribution~\cite{bernstein-book}. Core to the utility of a
database system is its ability to maintain application data that is
\textit{consistent}---that is, data that is well-formed according to
application semantics~\cite{gray-virtues}. By entrusting databases
with their data, applications are freed from the requirement to
manually manage this correctness. Maintaining consistency inherently
requires reasoning about and often controlling concurrent access to
data: in this section, we discuss this trade-off and describe classic,
conservative approaches to maintaining consistency.

\minihead{A simple example} As a motivating example we will revisit
throughout this paper, consider a simple payroll application managing
information about employees and departments. We will focus on three
specific features of the application:
\begin{myitemize}
\item\textbf{Employee IDs:} Employees are assigned ID numbers that
  should be unique with respect to all other assigned IDs (i.e., a
  primary key constraint).
  \item\textbf{Departments:} Employees should belong to exactly one
  department (i.e., a foreign key constraint). Employees can switch
  departments by updating their department assignment.
\item\textbf{Salaries:} Employees have salaries, and no employee
  should have salary greater than $\$50,000$.
\end{myitemize}
A database supporting the payroll application will have to be careful
in managing correctness as multiple users concurrently access the
database state. For example, if Stan is assigned ID number $5$ and
Mary is simultaneously assigned ID number $5$, then the application's
consistency will be compromised. On the other hand, properties like
the department constraint are easier to maintain. For example,
simultaneously adding Stan and Mary to the Engineering department is
safe. Effectively, some combinations of transactions and invariants
appear unsafe without coordination between concurrent operations,
whereas others appear to be resilient to independent access and
update.

A primary goal of this paper is to formalize this trade-off and state
a general property for deciding whether or not coordination is
required. More succinctly, we will answer the question: when does
correct transaction processing require synchronous coordination?

% traditional programmability: serializability; isolation is means
% towards achieving consistency

\minihead{Transactions and Isolation} The ACID transaction concept
pioneered by Jim Gray and System R relieves programmers of the
requirement to explicitly specify their consistency constraints by
encouraging the use of serializable
transactions~\cite{gray-virtues}. Under serialiable isolation, the
execution of a set of transactions is equivalent to some serial
ordering among them~\cite{bernstein-book}. As long as each transaction
leaves the database in a consistent state, serializable transactions
ensure database consistency. Accordingly, traditional database systems
treat isolation between concurrently executing transactions as a
\textit{means} towards achieving application consistency. Serializable
transactions are a \textit{sufficient} mechanism for ensuring
consistency but are not always \textit{necessary}: as a classic
example due to Lamport in
1976~\cite{lamport-audit,schneider-concurrent}, an ``audit''
transaction that monitors bank accounts for embezzlement does not need
to observe serializable state as long as account balances reflect
deposits and transfers.

% problem: serializability is actually pretty expensive; seen shift
% away from them

\minihead{Coordination costs} While serializability provides a
remarkably powerful and convenient abstraction, it is accompanied by a
hefty price tag: a requirement for
coordination~\cite{davidson-survey}. We formally define coordination
in Section~\ref{sec:model}, but, informally, we say that a database is
\textit{coordination-free} if each copy of shared database state can
can execute operations without contacting (and, therefore, possibly
stalling) other copies of state. This requirement has been captured in
distributed systems as \textit{availability}, or ``always-on''
operation: an available system can perform operations on any
non-failed server, despite arbitrary communication partitions between
them~\cite{gilbert-cap}. This also benefits normal operation: to serve
a request, a \cfree server need not contact any others~\cite{pacelc},
so client requests can safely proceed in parallel. In contrast, a
system that requires coordination (e.g., provides serializability)
faces unavailability in the presence of network partitions and partial
failures, and, during normal operation, incurs higher latency due to
communication delays~\cite{hat-vldb} and, possibly, resource
contention, unstable queuing effects~\cite{ladis}, deadlocks, and
spurious aborts~\cite{bernstein-book,gray-book,gray-virtues}.

% cost of coordination? unavailability, latency, stalls : focus on
% worst-case behavior yields average-case benefits

% benefit of coordination-freedom: infinite scalability

\minihead{Coordination and scalability} Most importantly,
coordination-freedom is intrinsic to scalable execution. A \cfree
system can scale without barriers: if the demands for a given resource
grow beyond that of a single computer, another computer can be added
to the system. The additional computer and the original (set of)
computer(s) need not synchronously coordinate, so adding more
computers results in a linear increase in capacity that can be
repeated indefinitely. While the term ``scalability'' is often abused,
coordination-freedom captures the essential property of a perfect
scale-out system, even for single-record operations.

% spectrum of models; actually infinitely many of them
% not necessarily even easy to program

\minihead{Alternative models} Given the costs of coordination, many
database designs and systems operators opt for weaker isolation models
that offer higher performance, lower latency, and fewer aborts. On a
single-node database, weaker models include Read Committed and
Repeatable Read isolation~\cite{adya-isolation}, while modern
distributed databases offer a range of isolation models such as
eventual consistency and regular register
semantics~\cite{hat-vldb}.\footnote{To prevent confusion, we will
  refer to distributed systems consistency models such as
  linearizability as \textit{isolation models} and reserve the use of
  \textit{consistency} for referring to application-level ``ACID''
  consistency.}  Not all weaker models are \cfree (e.g., Snapshot
Isolation)~\cite{hat-vldb}, but all expose end users to isolation
\textit{anomalies}, or behavior that could not have arisen in a serial
execution.

Unfortunately, determining whether weak isolation (and, moreover,
which isolation model) is safe for a given application is
difficult. Anomalies are often expressed in terms of (in-)admissible
traces of reads and writes, and the distinctions between models are
often subtle~\cite{adya-isolation,isolation-semantics} and vary
between implementations~\cite{hat-vldb}. Users must manually translate
from low-level traces to specific application behaviors, an
error-prone and laborious process, particularly for the non-specialist
developer~\cite{consistency-borders}. In the words of one senior
member of the database community, the usability consequences of
choosing weak isolation are tantamount to ``falling off a cliff.'' If a
developer chooses an incorrect model, she risks inconsistency or,
alternatively, extraneous coordination. More fundamentally, any choice
she makes ties her application implementation and database execution
strategy to a fixed isolation model, which may not maintain
correctness as application logic evolves or as multiple applications
concurrently access shared data. As a final concern, the proliferation
of isolation models and deployment of multiple systems to support
varying performance and isolation requirements (so-called ``Polyglot
Persistence'')~\cite{polyglot} hint that different operations---even
within a single application---need a \textit{combination} of
guarantees---one model does not fit all.

% dividing line: coordination---define them

% evidence for mixed models: polyglot persistence, adding support for
% CAS, basis for lock manager, etc.

\minihead{Correctness without coordination} We desire an alternative
that manages the trade-off between coordination and correctness while
reducing the tension between programmability and
performance. Applications should ideally execute with as little
coordination as possible, but non-serializable isolation anomalies can
and will result in inconsistency for arbitrary applications. We must
answer the question: given an application, which anomalies are
important? Rather than require application writers to manually
classify anomalies, we will instead formulate our criteria for
coordination in terms of application-level semantics. Given a set of
invariants describing application state (e.g., as part of the SQL
DDL), we will present a necessary and sufficient condition for
coordination-free execution. While this task requires some
formalization (Sections~\ref{sec:model},\ref{sec:bcc-theory}), we
demonstrate that it can yield pragmatic results both in languages like
SQL (Section~\ref{sec:bcc-practice}) and in real system deployments
(Section~\ref{sec:evaluation}).

% if you give up serializability, can't guarantee correctness in an
% arbitrary read/write model; users have to manage this trade-off for
% themselves

% here, consider a model where stored procedures are declared in
% advance, invariants are given to the database; goal will be to
% minimize synchronize coordination

