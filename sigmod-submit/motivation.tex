
\section{Coordination and Consistency}
\label{sec:motivation}

% application-level consistency is key requirement

As repositories for application state, databases are tasked with the
challenging goals of maintaining correct, durable data despite
concurrency, failures, and, often,
distribution~\cite{bernstein-book}. Core to the utility of a database
system is its ability to maintain application data that is
\textit{consistent}---that is, data that is well-formed according to
application semantics~\cite{gray-virtues}. By entrusting databases
with their data, applications are freed from the requirement to
manually manage this correctness. Maintaining consistency inherently
requires reasoning about and often controlling concurrent access to
data: in this section, we discuss this trade-off and describe classic,
conservative approaches to maintaining consistency.

% traditional programmability: serializability; isolation is means
% towards achieving consistency

\minihead{Transactions and Isolation} The ACID transaction concept
pioneered by System R relieved programmers of the requirement to
explicitly specify their consistency constraints by encouraging the
use of serializable transactions~\cite{gray-virtues}. Under
serialiable isolation, the execution of a set of transactions is
equivalent to some serial ordering between
them~\cite{bernstein-book}. As long as each transaction leaves the
database in a consistent state, serializable transactions ensure
database consistency. Accordingly, traditional database systems treat
isolation between concurrently executing transactions as a
\textit{means} towards achieving data consistency. Serializable
transactions are a \textit{sufficient} mechanism for ensuring
consistency but are not strictly \textit{necessary}: as a classic
example due to Lamport in 1976~\cite{lamport-audit}, an ``audit''
transaction over shared bank account balances need not observe
serializable state as long as no bank account balance it reads is
negative.

% problem: serializability is actually pretty expensive; seen shift
% away from them

While serializability provides a remarkably powerful and convenient
abstraction, it is accompanied by a hefty price tag. The requirement
that transaction execution conform to a serial order imposes penalties
on concurrent accesses to data items, resulting in aborts due to
deadlocks and decreased throughput due to
contention~\cite{bernstein-book,gray-book,gray-virtues}. The costs of
serializability in a distributed environment are even more
expensive---as we will shortly discuss, it is well known to be
provably unachievable without synchronous coordination between
replicas---due to the possibility of network partitions and
millisecond-or-higher cross-server latencies (up to hundreds of
milliseconds across datacenters)~\cite{hat-vldb,bobtail}.

% spectrum of models; actually infinitely many of them
% not necessarily even easy to program

Given the cost of serializability, many database designs and systems
operators opt for weaker models that offer higher performance, lower
latency, and fewer aborts. On a single-node database, these models are
often in the form of ``weak isolation'' such as Read Committed and
Repeatable Read isolation~\cite{adya-isolation}. Modern distributed
databases offer a range of data consistency models such as eventual
consistency and regular register
semantics~\cite{hat-vldb}.\footnote{To prevent confusion, we will
  subsequently refer to distributed systems consistency models such as
  linearizability as \textit{isolation models} (or, more simply,
  \textit{models}) and reserve the use of \textit{consistency} for
  referring to application-level ``ACID'' consistency guarantees. This
  is largely a product of scope: in a system where an end-user
  application is not considered (e.g., the traditional distributed
  systems literature), ``consistency'' is indeed best defined
  according to reads and writes on opaque registers.}  There are
infinitely many non-serializable models~\cite{hat-vldb}, but each
exposes end users to isolation \textit{anomalies}, or behavior that
could not have arisen in a serial execution. These anomalies
complicate reasoning about application behavior; as Gray and Reuter
pithily summarize: ``engineers can build distributed systems, but few
users know how to program them or have algorithms that use
them''~\cite{gray-book}. Users wishing to adopt one of these weaker
models must manually map their application-level consistency criteria
to the low-level traces of reads and writes that define each
alternative level---an error-prone and laborious process, particularly
for the non-specialist developer~\cite{consistency-borders}.

% dividing line: coordination---define them

\minihead{A Dividing Line: Coordination} While the literature contains
a large spectrum of isolation models, a fundamental property divides
the space: availability without coordination. We formally define
coordination in Section~\ref{sec:model}, but, informally, we say that
a database is \textit{coordination-free} if, given a copy of database
state, a user's operations can always proceed safely without
contacting (and, therefore, possibly stalling) other users
concurrently accessing shared state. This requirement has been
captured in the distributed systems community as
\textit{availability}, or ``always-on'' operation: an available
distributed system can perform operations on any non-failed server,
despite arbitrary communication partitions between
servers~\cite{gilbert-cap}. This focus on worst-case behavior yields
benefits during normal operation as well; systems that do not require
coordination can provide low latency: to serve a request, a server
need not contact any others~\cite{pacelc}, and client requests can
safely proceed in parallel. Over wide-area networks, this can
correspond to hundreds of milliseconds lower
latency~\cite{hat-vldb}. In contrast, a system that requires
synchronous coordination risks unavailability in the presence of
network partitions and partial failures, and, during normal operation,
incurs higher latency due to communication delays and, possibly,
resource contention.

% cost of coordination? unavailability, latency, stalls : focus on
% worst-case behavior yields average-case benefits

% benefit of coordination-freedom: infinite scalability

Most importantly, coordination-freedom is intrinsic to scalable
execution. A model that is achievable without coordination can scale
without barriers: if the demands for a given resource in a system grow
beyond that of a single computer, another computer can be added to the
system. The additional computer and the original (set of) computer(s)
need not synchronously coordinate, so adding more computers results in
a linear increase in capacity that can be repeated indefinitely. While
the term ``scalability'' is often badly abused, coordination-freedom
captures the essential property of a perfect scale-out system, even
for single-record operations.  Unfortunately, not all models are
achievable with coordination freedom. Serializability is provably at
odds with availability~\cite{davidson-survey}, as are useful models
like linearizability~\cite{gilbert-cap}---which provides real-time
guarantees on single data items---and Snapshot Isolation---a common
(weaker) replacement for serializability~\cite{hat-vldb}.

% evidence for mixed models: polyglot persistence, adding support for
% CAS, basis for lock manager, etc.

\minihead{Life with and without Serializability} In practice, the
trade-off between the convenience of ``strong'' isolation guarantees
and coordination-freedom is evidenced in common practice today. Some
applications require serializability for correct behavior, while the
many applications running on eventually consistent data stores are
evidence that synchronous coordination is not always required. Two
trends in particular highlight this requirement for a combination of
models. First, many deployments of weakly consistent stores are often
coupled with deployments of strongly consistent counterparts (e.g.,
Riak, Redis, and Postgres in a single web service stack). While this
``polyglot persistence'' is influenced by many factors including data
model and persistence format, availability and scalability are
frequenly mentioned as imporant criteria when choosing between
stores~\cite{polyglot}. Second, many databases today run at
non-serializable isolation by default and often as the strongest level
offered~\cite{hat-vldb}; even strongly consistent systesms like PNUTS
have added options for weaker isolation~\cite{pnuts-update}. On the
opposite side of the spectrum, recent database designs originally
intended for highly available, scalable operation have begun to add
stronger semantics (e.g., Riak and Cassandra independently added
support for compare-and-swap, a critical building block for mutual
exclusion and higher-level functionality like lock-based concurrency
control).
%https://blog.heroku.com/archives/2010/7/20/nosql

This use of mixed isolation models leads to the question: when is it
actually safe to forgo coordination (and therefore serializability)?
Applications should ideally execute with as little coordination as
possible, but non-serializable isolation anomalies can and will result
in inconsistency for arbitrary applications: the question becomes,
given an application, which anomalies are important? Our primary focus
in this paper is to answer this question with a necessary and
sufficient condition for coordination-freedom. To do so, we will also
have to directly consider application semantics. As we will discuss,
this requirement represents a burden for end-users, but we believe it
is preferable to have a user enumerate properties of her application
rather than reason about her application behavior at a lower level of
abstraction---namely, anomalies expressed the level of reads and
writes (as provided by most distributed isolation models).

% if you give up serializability, can't guarantee correctness in an
% arbitrary read/write model; users have to manage this trade-off for
% themselves

% here, consider a model where stored procedures are declared in
% advance, invariants are given to the database; goal will be to
% minimize synchronize coordination


\minihead{A simple example} As a running motivating example, we will
consider a simple payroll application managing information about
employees and departments. We consider three aspects of the payroll
database:
\begin{myitemize}
\item\textbf{Employee IDs:} Employees are assigned ID numbers that
  should be unique with respect to all other assigned IDs (i.e., a
  primary key constraint).
\item\textbf{Departments:} Employees should belong to exactly one of a
  (currently pre-declared, fixed) set of departments (i.e., a foreign
  key constraint). Employees can migrate departments at will by
  updating their department assignment.
\item\textbf{Salaries:} Employees have salaries, and no employee
  should have salary greater than $\$50,000$.
\end{myitemize}
Even without further consideration of query language or data layout,
there are several interesting properties of this application. Some
properties, such as ID uniqueness, may be violated in the event of
arbitrary updates: if Stan is assigned ID number $5$ and Mary is
simultaneously assigned ID number $5$, then application-level
consistency will be compromised. In this case, any replicas performing
insertions for a specific ID should coordinate. Other properties, such
as the department constraint, appear safe under concurrent insertions:
so long as the employee's department field corresponds to an existing
department, it appears (and, indeed, we will later prove) that
concurrent insertions can saefly proceed independently. However,
arbitrary operations like deletions \textit{can} compromise the
department constraint.

This example demonstrates a trade-off between transaction expressivity
and invariant strength: some combinations of transactions and
invariants appear unsafe without coordination, whereas others appear
to be resilient to independent access and update. The remainder of this paper
will formalize this trade-off and state a general
property for deciding whether or not coordination is required.
