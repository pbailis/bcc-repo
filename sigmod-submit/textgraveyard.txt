
\minihead{Life with and without Serializability} The trade-off between
the convenience of ``strong'' isolation and coordination-freedom
surfaces in common practice. Some applications seem to require
serializability, while many applications run on systems providing
weaker guarantees, like eventual consistency~\cite{vogels-defs}. Two
trends in particular highlight a requirement for a combination of
models. First, many deployments of weakly consistent stores are often
coupled with deployments of strongly consistent counterparts (e.g.,
Riak, Redis, and Postgres in a single web service stack). While this
``polyglot persistence''~\cite{polyglot} is influenced by many factors
including data model and persistence format, availability and
scalability are frequenly mentioned as imporant criteria when choosing
between stores. Second, many databases today run at non-serializable
isolation by default and often as the strongest level
offered~\cite{hat-vldb} (e.g., even strongly consistent systems like
PNUTS~\cite{pnuts} have added options for weaker
isolation~\cite{pnuts-update}). On the opposite side of the spectrum,
recent database designs originally intended for highly available,
scalable operation have begun to add stronger semantics (e.g., Riak
and Cassandra independently added support for compare-and-swap, a
critical building block for mutual exclusion and higher-level
functionality like lock-based concurrency control).
%https://blog.heroku.com/archives/2010/7/20/nosql

Using \iconfluence as test for coordination requirements exposes a
trade-off between the operations a user wishes to perform and the
semantic guarantees she wishes to guarantee about her data. At the
extreme, if a user's transactions cannot modify the initial database
state, she can guarantee any invariant, while, with no invariants, a
user can perform any operations. However, the space between yields a
spectrum of possible \iconfluent guarantees. In this section, we make
this trade-off concrete via a simple language analysis, a discussion
of system and merge function design, and a discussion of several
related techniques through the lens of \iconfluence.

\subsection{\lang: A Simple Example Language}

To illustrate the utility of \iconfluence, we consider a simple,
informal language modeled after simple SQL DDL and stored procedures,
called \lang. In \lang, databases store mutations in collections of
\textit{rows}, grouping rows with equivalent, pre-defined
\textit{columns} (fields) into \textit{tables}. Each column can
contain either \textit{string}, \textit{numeric}, or \textit{counter}
datatype. Strings and numeric types contain arbitrary (text or
numerical) data and support arbitrary modification, while counters
contain numeric data and support algebraic operations (e.g.,
\textit{increment}, \textit{decrement}). Users submit arbitrary
functions over data in the form of \textit{transactions}, each of
which consists of a variable number of sequential
\textit{statements}. Statements take the form of read (e.g.,
\textit{select}), write (e.g., \textit{insert}), or update (e.g.,
\textit{update}, \textit{increment/decrement} for counters,
\textit{delete}) operations.

To enable \cfreedom analysis, \lang allows users to define
\textit{invariants} over their tables that express application-level
consistency requirements. Many of these are borrowed from SQL. First,
a \textit{primary key} on columns $C_1 \dots C_n$ of table \textit{T}
requires that at most one one row should match each distinct
combination of possible values for the specified columns. Second, a
\textit{autoincrement} constraint on a primary key of numeric type
requires that numeric values be contiguous. Third, a \textit{foreign
  key} on numeric columns $C_F=\{C_{f1}\dots C_{fn}\}$ of table $T_f$
referencing another set of columns $C_R = \{C_{r1}\dots C_{rn}\}$ (resident in a
different table $T_f \neq T_r$) dictates that, for each row in $T_f$,
there should be at least one row in $T_f$ whose values for $C_r$
matches that of $C_r$. Unlike SQL, \lang adds additional constraints
to each column: \textit{not equals}, \textit{equals}, and, for numeric
and counter datatypes, \textit{less than}, \textit{greater than}, and
\textit{sum\_of} (the final representing an aggregate of multiple
other numeric columns).



With this simple (but expressive) language in mind, we can enumerate a
set of \iconfluent and non-\iconfluent operations. We do not claim
that this enumeration is complete or that \lang is sufficiently
expressive for all applications. However, as we will see
(Section~\ref{sec:evaluation}), it is surprisingly expressive for
many.

Several operations are \iconfluent: any operation on columns without
constraints, increment (decrement) of counters without \textit{less
  than} (\textit{greater than}) constraints, modification of foreign
key columns (provided $i.)$ a suitable remote data column is read as
part of the same transaction or $ii.)$ a suitable remote data column
is modified), and any (in-)equality constraints, \textit{sum\_of}
(with the same caveat as foreign key constraints). Given cluster-wide
unique ID generation (i.e., UUID or via cluster membership), insertion
into primary key columns without autoincrement constraints (provided
the columns are not specified by the end user---e.g., ``give me a new
ID'' versus ``insert this new ID'') are also supported. In contrast,
insertion of specified primary keys, use of autoincrement, and
\textit{less than} and \textit{greater than} for decrement and
increment operations, respectively, are not invariant
commutative. Perhaps not surprisingly, all of these cases can be
checked by simple syntactic rules; we built a simple \lang analysis
tool that identifies all of the above constructs in addition to
limited support for conditional updates in less than a week.

\subsection{Dealing with Conflicts}
\label{sec:conflicts}

If a set of transactions is not \iconfluent, then concurrently
executing combination of the transactions might violate the given
integrity constraint. This requires coordination--but how much?

At one extreme, it is sufficient to perform (serial) mutual exclusion
between any possibly conflicting transactions. For instance, if
procedures $p_1$ and $p_2$ could conflict, then a system could execute
each under a serializable isolation level. This is expensive: for any
\textit{possible} conflicts, transactions will have to coordinate and
potentially block, even for operations in the transactions that do not
conflict.

We suggest an alternative: let transactions execute in isolation and
produce outputs, and then use optimistic concurrency control to check
whether any conflicting outputs were actually produced. This
validation step is possibly expensive, as \textit{every} possible
conflict must be checked. If $\delta_{i1}$ and $\delta_{j1}$ conflict,
any transaction producing either of these outputs must contact a
validator in order to check whether the corresponding conflicting
action has been performed. If so, we have two options. In the basic
case, the transaction must abort due to a would-be conflict of
integrity constraints. We call these updates \textit{abortable
  conflicts}. However, not all conflicts require aborting. For
example, a transaction that enforces that \textit{some} doctor is on
duty need not actually abort if an alternate doctor is actually on
staff. We model these \textit{non-abortable conflicts} as
mini-transactions---effectively, closures that operate atomically on
database state. In the doctor case, the mini-transaction might check
the table for an existing doctor and simply produce no new output. In
a less trivial case, a user might autoincrement an ID counter when
inserting a new row; non-abortable conflicts only require
serialization, not aborts. Of course, this problem of reducing
conflicting operations introduces a new set of challenges.

Fortunately, the problem of minimizing conflicts is well-studied
(Section~\ref{sec:relatedwork}). The most aggressive decomposition
techniques we have encountered are those of Bernstein and Lewis's
``Assertional Concurrency Control'' protocols, which decompose
transactions into minimally-sized atomic units that are subsequently
executed as to avoid pre-condition invalidation (cf. \textit{maximally
  reduced proofs} for non-modular transaction
decomposition~\cite{decomp-semantics}). To do so requires additional
semantics about intermediate pre- and post-conditions for each
operation: this brings this research into the realm of programming
languages. We believe this is a worthwhile area for future work, but,
due to programmer burden, do not further consider these techniques in
this paper.

\subsection{Merge Functions}
\label{sec:merge}

So far, we have assumed a simple bag representation of database
state. This is convenient for proving properties about database state
and reasoning about the behavior of merge functions, but, in practice,
merges may be more complex. For example, for many applications, it is
impractical to retain all versions ever written to the
database. Instead, databases may wish to \textit{compact} these
versions to limit storage requirements. A simple compaction policy is
\textit{last writer wins}, whereby writes are assigned a timestamp and
only the highest-timestamped write to each time is
retained~\cite{dynamo}. However, improper use of last-writer wins can
yield \iconfluent but functionally inconsistent results: consider, for
example, a user that wishes to ensure that no bank account has
negative balance. If using a simple numeric register, if two users
each withdraw \$60 from an account with \$100, a last-writer wins
column might end up with \$40 total. This can be prevented by adding
an additional invariant that states that account balances should
reflect all additions and removals from each account, but this is an
arguably non-trivial pitfall.

We propose the use of abstract data types to avoid anomalies such as
the above. The bank account example is not technically incorrect, but
it reflects an incomplete specification of invariants. In many
real-world examples, users' successful operations should be reflected
in any converged database state; by choosing merge functions that
reflect this ``durability'' requirement, a database can avoid these
odd anomalies. While many practitioners today must hand-code their own
merge functions (e.g., Bayou~\cite{bayou} and Dynamo~\cite{dynamo}
conflict reconciliation), we advocate a model in which a
\textit{library} of type-specific merge functions are presented to
users. This approach---suggested by concurrency control for abstract
data types~\cite{weihl-thesis} and, more recently, Commutative and
Replicated Data Types~\cite{crdt}---simplifies the task of merging
updates while also reducing the overheads of operations. Unlike this
prior work, however, \iconfluence analysis \textit{also} guarantees
the correctness of (specified) arbitrary transformations on these
types, thus eliminating the problem of Conway et al.'s
\textit{scope dilemma}~\cite{blooml}, whereby operations on individual
data items are valid but their composition is not.

\input{isolation-subsec}

\input{confluence-subsec}



The past decade has seen increased tumult in the landscape of database
design: faced with increasing scale and requirements for always-on
operation, many service operators shifted away from traditional
database designs and semantics and towards alternatives offering
greater scalability, availability, and
performance~\cite{dynamo,bigtable,cassandra,brewer-base}. This trend
belies a \textit{fundamental} divide in the design space of database
systems: the need for synchronous coordination between concurrent
operations. On the one hand, ``strong'' data consistency (i.e., CAP
``CP''~\cite{gilbert-cap}) models such as the gold standard of
traditional database concurrency control---the serializable
transaction, guaranteeing ``single system image''
behavior~\cite{gray-virtues}---provide semantic guarantees that are
useful for programmers~\cite{bernstein-book}. However, these
``strong'' models require synchronous coordination across replicas in
order to provide a safe response, leading to provable unavailability
and increased latency, and, in a single-site context, possible stalls
during execution~\cite{pacelc}. In contrast, ``weak consistency''
(i.e., CAP ``AP'', or ``highly available''~\cite{gilbert-cap}) models
provide a subset of the semantic guarantees but do not require
synchronous coordination and therefore do not suffer from
unavailability or latency~\cite{hat-vldb}. The latter category of
models (and many of their implementations~\cite{swift}) defer
coordination until some future point~\cite{calm,consistency-borders}
and are therefore are perfectly scalable, even at the granularity of
an individual database record. However, in giving up semantic
guarantees, weak isolation models sacrifice intuitive programmability.

This provable trade-off between semantics and coordination leads
programmers to a difficult choice. On the one hand, many applications
cannot operate correctly without serializable or Snapshot Isolation
guarantees. However, on the other hand, many applications---whether
running on newfangled stores providing weak data consistency or under
traditional ``weak isolation'' guarantees~\cite{adya-isolation}---seem
to tolerate weaker guarantees. Ideally, users can choose the weakest
possible set of semantics and still maintain correctness, but this is
no easy task: programmers today must manually map their
application-level consistency concerns to low-level models expressed
via constructs like admissible traces of reads and writes. This in
turn requires developers to become experts in distributed data
consistency, weak isolation, and replication
protocols~\cite{consistency-borders}.

In this paper, we seek an alternative approach that allows users to
determine when it is provably \textit{necessary} to employ
coordination in order to maintain application-level
correctness. Foregoing synchronous coordination (and therefore
serializability) necessarily means that users will be exposed to
isolation \textit{anomalies} resulting from concurrent
access. Determining which anomalies matter to an application is
impossible if a database system has no knowledge of the
application. Accordingly, we consider a model in which users
explicitly provide the database with \textit{integrity constraints},
or predicates representing application-level consistency that should
always be satisfied by the database state. Given these constraints,
the database can determine which procedures will require synchronous
coordination and, often, which read/write data consistency and
isolation level each procedure requires. We prove that the
\textit{\fullnameconfluence} property
(\iconfluence)~\cite{obs-confluence}---informally, the ability to
merge isolated transaction executions without violating
constraints---is a necessary and sufficient condition for executing
with \cfreedom. While we believe that this is the first necessary and
sufficient condition for availability without coordination, our
results leverage several decades-old concepts from the database
literature, including semantics-based concurrency
control~\cite{sdd1,decomp-semantics,badrinath-semantics,garciamolina-semantics,korth-serializability}
synchronization for abstract data
types~\cite{herlihy-apologizing,weihl-thesis}, and nested atomic
transactions~\cite{atomictransactions}
(Section~\ref{sec:relatedwork}).

This theory provides a foundation for \textit{coordination-avoiding
  databases}: systems that coordinate only when necessary.  If a
coordination-avoiding database system coordinates, it is because
application consistency \textit{cannot} be guaranteed without doing
so. Accordingly, coordination-avoidance analysis directly captures the
scalability of a given application: an application with \cfreedom can
effectively scale infinitely with the addition of more machines, while
the degree to which conflicts are distributed directly determines an
application's ability to scale out and up. We pithily summarize the
philosophy of coordination-avoidance via two high-level principles:
\begin{introenumerate}
\item \textit{Let concurrency safely flourish}: if a user's operations
  are \iconfluent, they should be executed on separate copies of
  database state, without coordination.
\item \textit{Minimize the distribution (both in space and time) of
  conflicting operations}: if a user's operations do not commute,
  execute the conflicting sections while involving as few servers and
  with as short of a critical section as possible.
\end{introenumerate}
To illustrate the utility of these principles, we embody \iconfluence
analysis in a tiny language, \lang, and analyze applications for
\cfreedom. As a proof of concept, we analyze and implement the TPC-C
New-Order on a coordination-avoiding database prototype and achieve
linear scalability to over 1.8 million transactions per second on a
100-node EC2 cluster (nearly four times the current record held by
Oracle). Our prototype's competitive advantage is not due to
bleeding-edge performance-oriented engineering but is instead due to
theoretically-motivated, judicious use of coordination.

Overall, this paper attempts to improve distributed database
programmability by unifying industry trends with the collected wisdom
of the database community. We recognize the scalability advantages of
weakly consistent data stores but reconcile them with the
programmability benefits of maintaining traditional ACID Consistency
on behalf of application designers. In doing so, we place an
additional burden on programmers, who must either provide the database
with a complete and machine-interpretable specification of
application-level invariants or otherwise must perform analysis
themselves and manually label their transactions. For programmers that
wish to achieve their application's maximum scalability, we believe
that both of these alternatives are far preferable to the
state-of-the-art ad-hoc mapping from application-level ACID
Consistency to distributed data consistency models performed today. We
accordingly view this work as the first step in revisiting core
database concepts like query optimization, failure recovery, and data
layout in light of increased knowledge of application-level semantics.


\begin{table*}
\begin{tabular}{|c|c|c|}
\hline
Constraint & \iconfluent Operations & Non-\iconfluent Operations \\\hline
\texttt{PRIMARY KEY} & modifications without specified values & modification with values specified \\
\texttt{AUTOINCREMENT} & - & insertion \\
\texttt{FOREIGN KEY} & mutation with pairing & mutation without pairing, deletion\\
\texttt{!=, ==} & all operations & - \\
\texttt{<} & increment, modification & decrement \\
\texttt{>} & decrement, modification & increment\\\hline

\end{tabular}
\caption{\iconfluence for \lang.}
\label{table:invariants}
\end{table*}


\footnote{This problem is more traditional due to two
  reasons. First, it assumes a mechanism to execute several operations
  atomically (serially), which requires coordination; here, we have
  instead (largely) focused on determining when coordination is
  required at all. Second, this work typically (but not exclusively) reasons
  about concrete transaction executions rather than \textit{all}
  possible executions.}

\miniheadnostop{Why specify stored procedures?} Much of the database
literature assumes a model in which users execute arbitrary queries
composed of arbitrary groupings of operations that are not known in
advance. While this model is well suited to the manual traditional
data processing and data entry tasks envisioned by early database
pioneers (i.e., users sit at terminals and type in transactions one by
one), there are at least two reasons why stored procedures are an
appropriate choice today. First, the large query volumes driving
today's distributed database designs are overwhelmingly created by
machines, not humans. Indeed, individual procedure invocations may
differ, but the overall structure of a queries at scale are often
known in advance, whether encoded in SQL or, alternatively,
application logic in an application server or ORM system. Second,
eliminating humans from transaction processing loop reduces execution
time and therefore the impact of critical sections. Several well-known
serializable systems such as H-Store and VoltDB, Calvin, and Granola
all exploit this a-priori knowledge to great effect. It \textit{is}
possible to support general purpose read/write transactions on opaque
registers in our formalism, but the results are not likely to be
useful.\vspace{.5em}

In the case of partially
replicated databases, whereby no one server contains an entire copy of
database state, we say that a database does not require coordination
if, whenever a transaction can access a replica for each of its
operations, those replicas guarantee transactional availability for
the transaction without contacting another equivalent set of replicas.


---

Conway's Law: combinations of operations and invariants commute: e.g., all write, no read, all read, no write

----

Applying analysis to systems:

Building a custom language is exciting but ultimately has performance challenges

Today, most immediate impact: programmers can manually reason about their application constraints without thinking about low-level models like causality and eventual consistency; also, smarter datatypes in the database like commutative counters and so on...

Low bar for a ``BCC'' system: each sp invocation (transaction) comes annotated with labels for either commuting or, if not, next hop in cycle (it's possible to collapse hops, but not strictly necessary); DB still doesn't know anything about stored procedures or integrity constraints

High bar for ``BCC'' system: all sp known in advance, all integrity contstraints know in advance

Our prototype focuses on the low bar for now---the high bar ventures into the domain of specialized program analysis. As we discuss in FUTUREWORK, we've had success in building small languages to capture the requirements of EVALUATION but reserve a full discussion for future work.

---

Merge function

There are some dumb answers: merge = nil; merge = LHS

Some better answers:
``bag'' semantics, expose all versions -- hard for the programmer, but effectively what ``immutability'' argument is all about

---

SIMPLE LANGUAGE FOR ANALYSIS

What commutes?

for now, assume we have data types that are known in advance: blobs/strings, numbers, counters

assume: PKEY, FKEY, UNIQUE, AUTOINCREMENT, NOGAP, !=, <, >

COMMUTES: 

counter.inc() and >
counter.dec() and <

any kind of !=
FKEY

with nonce, PKEY insert without specifying key
AUTOINCREMENT not sequential

CONFLICTS:

AUTOINCREMENT with NOGAP (sequential)
counter.inc() and <
counter.dec() and >
DELETE and insert into FKEY column


DISCUSSION: not claiming completeness (at this point), but, for a
simple SQL-like interface, this is actually pretty easy to enumerate
and, for simple programs, check. recursive SQL and unbounded loops
face the same problems, but, for the queries we've looked at, not
horrible.


In this paper, we seek an alternative solution: allow the use of
coordination-free protocols whenever they are safe and only require
coordination when it is provably required to do so. Our goal is to let
users forego strong models like serializability whenever possible but
still maintain application correctness. Given a system that can only
reason about arbitrary operations on opaque read/write registers, this
is an impossible task: giving up serializability necessarily means
that users will be exposed to a range of isolation \textit{anomalies},
or artifacts due to concurrent data accesses. To understand which
anomalies are acceptable, we require application writers to inform the
database about their application's notions of correctness in the form
of declarative integrity constraints. This in turn allows the database
to perform the compilation from application-level concerns to
distributed coordination mechanisms like linearizable test-and-set and
eventually consistent writes.


The past decade has seen increasing tumult in the landscape of
mainstream distributed database design.  Faced with increasing scale
and requirements for always-on operation, many service operators
shifted away from traditional database designs and semantics. The gold
standard in traditional database concurrency control---the
serializable transaction, which guarantees ``single system
programmability'' over arbitrary read-write operations---was provably
unachievable under the Internet services' requirements for high
availability and low latency. Instead, operators largely forfeited
transactional semantics in favor of ``weak'' consistency models. As of
2013, the long-term impact of this movement is unclear: we have
recently seen a resurgence of interest in transactional models, with
considerable debate as to which of these semantics---or possibly
alternative semantics---will emerge victorious.

These shifts nonetheless underscored a more fundamental divide in the
design space of distributed databases: requirements for
coordination. The introduction of the CAP Theorem in 2000 exposed many
modern practitioners to the trade-offs between strength of semantic
guarantees and coordination. On the one hand, ``strong'' data
consistency (i.e., CAP ``CP'') models provide semantic guarantees that
require coordination across replicas, leading to unavailability and
increased latency. In contrast, ``weak consistency'' (i.e., CAP
``AP'', or ``highly available'') models provide a subset of the
semantic guarantees but do not require synchronous coordination and
therefore do not suffer from unavailability or latency. The latter
category of models (and many of their implementations) are perfectly
scalable, even at the granularity of an individual database
record---all coordination can be safely deferred until some future
point (and, under some models, forever). However, in giving up
semantic guarantees, weak isolation models sacrifice programmability.

Data store users today must choose between two options: accept
expensive coordination via strong semantics or, alternatively, reason
about whether low-level weak consistency primitives are sufficient for
their applications. As Gray and Reuter pithily summarize: ``engineers
can build distributed systems, but few users know how to program them
or have algorithms that use them.'' Programmers using modern, highly
scalable data stores are not only frequently exposed to various
isolation and data consistency anomalies but must, in many cases,
effectively become distributed systems experts in order to reason
about system behavior during communication delays, concurrency, and
failures. Moreover, as evidenced by the proliferation of ``polyglot
persistence'' and the introduction of optional linearizable operations
in traditionally weakly consistent data stores, applications need a
mix of strong and weak models. Managing this trade-off is in turn
challenging: with too much strong consistency, scalability will
unnecessarily suffer, while too little strong consistency may
sacrifice correctness.




We rigorously justify these simple intuitions with theory. The first
condition requires the notion of \textit{invariant commutativity},
which we show is a necessary and sufficient condition for distributed
execution without coordination. Using this mechanism, if users provide
the database with a set of stored procedures and declarative
invariants over database state, we can provably determine when
invariants might be violated. Unlike prior mechanisms, this only
requires the user to provide a single set of invariants, without pre-
and post-conditions for each transaction
(Section~\ref{sec:relatedwork}). However, When operations are not invariant
commutative, synchronous coordination is required, but this does not
mean that the \textit{entire} transaction requires synchronous
coordination. Rather, by analyzing conflicting operations within a
transaction---which we perform via a symbolic variant of transaction
chopping---and performing careful data layout, we can minimize
coordination. We largely focus on distributed execution, but these
techniques are also applicable to single-node databases (and their
``weak isolation'' models).

BCC exposes coordination requirements that are fundamental to a given
application.




, or commute---and providing
\textit{countersteps} to define appropriate compensation logic. Our
definition of semantic commutativity formalizes the notion of
compatibility sets at the level of individual transactions and across
replicas, with no requirement for users to specify countersteps. We
require users to specify the space of transactions they wish to
execute, obviating the need for user intervention via fine-grained
labeling of transaction types. As a final important distinction, while
Garcia-Molina assumes each transaction \textit{step} is executed
atomically (linearizably), this requires coordination across replicas,
so semantic commutativity explicitly accounts for (temporarily)
divergent copies of database state.

 Semantic commutativity
generalizes this notion to include all possible interleavings of a
known set of transactions. Garcia-Molina proposes the use of
compatibility sets describe valid interleavings of individual, atomic
(node-local) operations, while semantic commutativity analysis uses a
single invariant to describe interleavings across multiple
nodes. Garcia-Molina considers countersteps to roll back faulty
transactions. No requirement here for internode consistency
constraints. Open question: ``will performance really improve?'' here,
we show definiteively yes.

Degenerate form of Owecki-Gries ``interference freedom'', where each
transaction's pre-condition and post-condition are the same:
invariants are satisified. This is only possible because the database
is free to abort transactions whose committing would violate integrity
constraints. Accordingly, Owecki-Gries and related concurrent
verification techniques may prove useful in \textit{proving} invariant
commutativity but require modification to handle internal aborts and
multi-statement transactions. We are actively investigating these
techniques as we expand our BCC analysis.

Long history spanning distributed databases, long-running
transactions, and semantics-based concurrency control.


Another important distinction between BCC and related work on
increasing concurrency is that BCC analysis flags \textit{any}
possible non-commutative operation for coordination. This is
conservative---for example, two decrement operations may not actually
cause a negative bank account balance---but, given the possibility of
violation, replicas must coordinate. Prior research leveraged
commutativity as a basis for increasing
concurrency~\cite{herlihy-apologizing,
  weihl-thesis,predicatewise-serializability} for individual
transactions, requiring coordination (therefore sacrificing
availability and low latency) but increasing concurrency nonetheless.

parallelizing compilers~\cite{rinard-compiler}


Alternatively, can apologize (Sagas).  Here, we enforce invariants
without compensation code.

``Local Verification of Global Integrity Constraints in Distributed
Databases''

closely related to detection of weak conjunctive global predicates
(``possibly'', equivalent to $K_i$ local knowledge), symbolic variant
of




Slightly less expensive is to only execute potentially conflicting
operations serially: any side effects that do not conflict can be
applied. However, need to make sure that transaction does not cause
abort. So, resolve conflicts then atomically apply all effects in
transaction (via MAV/Read Atomic semantics), as in OCC.

Now give the straw-man of having more semantic information between
steps (i.e., non-modular decomposition).

So far, have assumed that side effects are pre-determined, but many
``conflicts'' can be avoided if executed serially. Consider
AUTOINCREMENT. Really just needs to be executed atomically. So, in
effect, just need to serialize those operations. Consider it a
modification \textit{intent}: $intent(read\_set) \rightarrow
\{\delta_1,\dots,\delta_n}$. Pretty straightforward now: can perform
  transaction chopping on read, write sets at runtime. Steps become:
  execute abortable updates, perform chopping, apply rest of the
  updates. Note that this is expensive, but we do not attempt to find
  an optimal solution (e.g., could coalesce the abortable updates with
  the chopping state, although the ``abort'' case Chopping resolves
  first). Instead, simply trying to find guidelines---practically
  apply them in the next section.

Of course, even chopping will be conservative, but the problem is
that, within a transaction, you want to violate invariants. Therefore,
two choices: be conservative or request more fine-grained assertional
semantics from the end user. Here, we consider the former strategy but
refer the interested reader to



Modular: holds over all possible conflicts
Non-modular: holds over transaction state; precondition goes beyond I

Coalesce transactions to minimize *residual* interference; but don't want to over-coalesce
Need to derive the ``weakest assertions''--find a ``maximally reduced proof'';
assertional concurrency control scheme for system checks each transaction

-----

If a set of stored procedures is not invariant commutative, it does
not necessarily mean that all procedures must be executed with
coordination. If a subset of a procedure's outputs conflict, only
those conflicts must be executed.

We construct an invariant-conflict graph as follows:

For each pair of procedures $p_1$, $p_2$ such that there exists a
database state $D$ such that $I(p_1(D)\rightarrow \Delta_1)$ and
$I(p_2(D)\rightarrow \Delta_2=\{\delta_{2,1},\dots\delta_{2,n}\})$ are
true but $I(p_1(D) \cup p_2(D))$ is false, add an edge between
$\delta_{1,i}$ and $\delta_{2,j}$ whenever $I(p_1(\Delta_1-\delta_{1,i})
\cup \Delta_2-\delta_{2,j})$ is true.

Conservative estimate: run all parts of transaction serially
Less conservative estimate: run conflicting parts serially

Even less conservative: distinguish between merges and aborts

Abortable conflicts: resolve serially
Mergeable conflict: resolve afterwards -- mutation as *intent*, not literal

1.) resolve aborts
2.) resolve mergeable conflicts
3.) apply the rest of transactions

\begin{theorem}

\end{theorem}


\minihead{Invariant Specification} One limitation of our current
approach is that it requries users to fully specify any invariants
that they are concerned with. Without invariants, we assume that all
queries are \iconfluent and can be executed with \cfreedom. An
alternative approach would start with an assumption that no
transactions are \iconfluent and would execute all transactions under
serializable isolation. Users could subsequently annotate their
transactions as \iconfluent and enjoy the benefits of scalability as
\cfreedom is recognized. We view these two approaches as complementary
and, a complete coordination-avoiding system should easily accomodate
both. Both approaches maintain the beneficial property that users
reason about their applications (via invariants) rather than low-level
isolation anomalies; indeed, the requirement for a full specification
replaces the requirement for a full analysis of all possible
read/write traces.


\minihead{Lifecycle Management} A common end-user requirement that has
proven particularly important for real-world users is the ability to
reason about new invariants as they are introduced into
already-deployed databases. For example, if a user wants to add a
given invariant/constraint, how will it impact running queries and
other invariants in the system? As a baseline concern, we must ensure
that the constraints in the database are satisfiable: a configuration
with invariants that are not simultaneously achievable will become
unavailable and, worse, represents inconsistent application
state. This is manageable via standard (if expensive) satisfiability
analyses but, more importantly, also means that the addition of new
invariants cannot be performed in a \cfree manner. Towards the
question of impact on running queries, we believe that lightweight
logging of transactions will allow analysis of new queries as they are
added to a database and, in general, can allow users to directly
assess the scalability effects of (adding \textit{or} removing) a
given invariant. In summary, we believe that coordination-avoidance
via invariant specification is achievable in a sane operational manner
but will require care in ensuring that the addition of new invariants
does not compromise database integrity (or, if desirable, performance
and availability).

\subsection{From Applications to Isolation Models}

Even if an application is \iconfluent and therefore achievable with
coordination-freedom, application designers (or, alternatively, a
database query planner) may still wish to choose an appropriate
isolation model. There are two primary decisions a system faces: how
should new updates be made visible (with respect to other updates),
and how quickly should updates be made visible (with respect to real
time)?

\minihead{Making updates visible} Ideally, users could instead use
\textit{no} concurrency control or \textit{eventual consistency} to
disseminate their \iconfluent updates. However, this can lead to
anomalies: for example, applying multi-item writes without concurrency
control might result in a situation where readers observe some writes
but not the others, violating a declared foreign key
constraint. Towards this goal, two models are particularly useful. The
first, the \textit{happens-before} relation from
causality~\cite{lamportclocks} informally ensures that, if a write is
visible, the writes that influenced the write are also visible. This
is useful in the case that a user reads a write (e.g., read
\textit{department.id=20}) and one of her subsequent writes depends on
that write (e.g., insert \textit{user.department=20}). We can use a
degenerate form of happens-before---so-called \textit{explicit
  causality}~\cite{explicit-socc2012} to ensure that, if a user issues
a write to an item that explicitly (via a forign key dependency)
depends on a previously write, the latter is only observed with the
former. The second model, \textit{Read Atomic isolation}
(RA)~\cite{ramp-txns} ensures that, once one of the writes in a
transaction is visible to a reader, all are visible (e.g., a single
transaction inserts \textit{department.id=20} and
\textit{user.department=20}).

Given our \cfree model of divergent replicas with merges, if merge
respects explicit causality (defined with respect to $I$) and RA
isolation, then $I$ will not be violated. Given that causal
consistency is known to have substantial metadata overheads (which
explicit causality mitigates to some extent~\cite{explicit-socc2012})
and that the best known implementation of RA
isolation~\cite{ramp-txns} requires two rounds of communication for
reads and writes, these models are best employed only when
necessary. In \lang, we need only employ them in the case of foreign
key updates.

\minihead{Controlling recency} While individual replicas may respect
invariants, users often desire \textit{recency} guarantees on their
updates: guarantees on the visibility of their updates, either to
their own future transactions (e.g., read-your-writes guarantees) or
to other users (e.g., linearizability). A class of guarantees from the
distributed systems and database literature called \textit{session
  guarantees}~\cite{bayou} enforces the former, often via so-called
\textit{sticky availability}: all of a user's transactions are
executed against the same (logical) copy of the
database~\cite{hat-vldb}. This satisfies common requirements like
reading one's prior writes and can be used to implement more complex
models such as PRAM~\cite{pram} and causal~\cite{lamportclocks}
recency guarantees. The latter (more generally, global real-time
recency guarantees) may require coordination. A bounded-staleness
guarantee can indeed be provided with periodic broadcasts between
replicas: in the failure-free case, this is \cfree with respect to
individual requests, but, in the presence of failures, may sacrifice
availability. More stringent guarantees like linearizability will
require coordination for updates. As Bailis et al. note, these
distributed systems guarantees are largely orthogonal to traditional
ACID semantics~\cite{hat-vldb} (e.g., traditional serializability does
not place any recency guarantees on cross-transaction recency) but are
often of interest to practitioners (typically, in weaker forms like
read-your-writes).


\subsection{Beyond Confluence}

A range of techniques have been proposed to mitigate the trade-off
between consistency and coordination. We defer a full survey of the
field to Section~\ref{sec:relatedwork} but briefly discuss three in
detail here.

\minihead{Escrow} 
\minihead{Immutability} Immutablilty has long been touted as a
strategy for improving the ability to reason about and program
distributed systems~\cite{helland-immutable,gray-virtues}. Indeed,
immutability eliminates the problem of handling namespace collisions
(either by pre-allocating or otherwise coordinating in order to manage
the space of IDs). However, immutability is subject to many of the
same problems as non-\iconfluent operations. For example, if we store
bank deposits and withdrawals in a ledger instead of perfoming commutative
update-in-place (as is encouraged in \lang), we have not actually
solved the problem of ensuring that no account has negative
balances. Instead, what immutability provides is a simple
\textit{merge} procedure: without namespace conflicts, no write is
lost due to, say, improper reconciliation techniques. This is a
powerful property but it is not sufficient to prevent true
application-level consistency violations.

\minihead{Monotonicity} Hellerstein's CALM conjecture~\cite{calm}
(subsequently proven as the CALM Theorem~\cite{ameloot-calm})
establishes a strong connection between monotonic logic and
confluence. CALM states that, if applications are restricted to
monotonic logic, they will produce deterministic output despite
reordering of execution. This indeed precludes non-\iconfluent
behavior such as subtracting from bank account balances, but it is
restrictive in at least two ways. First, determinism is often not
necessary as long as application-level invariants are satisfied: take,
for example, our audit and bank account balance from
Section~\ref{sec:bcc-theory} (alternatively, as a classic example from
distributed computing, consensus objects require that exactly one
value is chosen, not that a \textit{particular value} is chosen---this
is core to the problem specificiation, where it is called
\textit{non-triviality}). Users may wish to perform non-monotonic
logic, and, as we have seen, non-monotonic logic is sometimes safe to
execute---trivially, if there are no invariants that correspond to the
output of the non-monotonic operations. Second, deterministic outcomes
are not necessarily correct with respect to application-level
invariants. If a user issues only \textit{increment} operations on a
distributed counter, then, indeed, the program is monotonic and will
be deterministic despite re-ordering of increment operations. However,
if the desired behavior is to maintain a specific counter value,
monotonicity of the program actions is insufficient to guarantee
correctness.
