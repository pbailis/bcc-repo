
\section{From Theory to Practice}
\label{sec:bcc-practice}

Using \iconfluence as test for coordination requirements exposes a
trade-off between the operations a user wishes to perform and the
semantic guarantees she wishes to guarantee about her data. At the
extreme, if a user's transactions cannot modify the initial database
state, she can guarantee any invariant, while, with no invariants, a
user can perform any operations. However, the space between yields a
spectrum of possible \iconfluent guarantees. In this section, we make
this trade-off concrete via a simple language analysis, a discussion
of system and merge function design, and a discussion of several
related techniques through the lens of \iconfluence.

\subsection{\lang: A Simple Example Language}

To illustrate the utility of \iconfluence, we consider a simple,
informal language modeled after simple SQL DDL and stored procedures,
called \lang. In \lang, databases store mutations in collections of
\textit{rows}, grouping rows with equivalent, pre-defined
\textit{columns} (fields) into \textit{tables}. Each column can
contain either \textit{string}, \textit{numeric}, or \textit{counter}
datatype. Strings and numeric types contain arbitrary (text or
numerical) data and support arbitrary modification, while counters
contain numeric data and support algebraic operations (e.g.,
\textit{increment}, \textit{decrement}). Users submit arbitrary
functions over data in the form of \textit{transactions}, each of
which consists of a variable number of sequential
\textit{statements}. Statements take the form of read (e.g.,
\textit{select}), write (e.g., \textit{insert}), or update (e.g.,
\textit{update}, \textit{increment/decrement} for counters,
\textit{delete}) operations.

To enable \cfreedom analysis, \lang allows users to define
\textit{invariants} over their tables that express application-level
consistency requirements. Many of these are borrowed from SQL. First,
a \textit{primary key} on columns $C_1 \dots C_n$ of table \textit{T}
requires that at most one one row should match each distinct
combination of possible values for the specified columns. Second, a
\textit{autoincrement} constraint on a primary key of numeric type
requires that numeric values be contiguous. Third, a \textit{foreign
  key} on numeric columns $C_F=\{C_{f1}\dots C_{fn}\}$ of table $T_f$
referencing another set of columns $C_R = \{C_{r1}\dots C_{rn}\}$ (resident in a
different table $T_f \neq T_r$) dictates that, for each row in $T_f$,
there should be at least one row in $T_f$ whose values for $C_r$
matches that of $C_r$. Unlike SQL, \lang adds additional constraints
to each column: \textit{not equals}, \textit{equals}, and, for numeric
and counter datatypes, \textit{less than}, \textit{greater than}, and
\textit{sum\_of} (the final representing an aggregate of multiple
other numeric columns).

\begin{table}
\definecolor{yesgray}{gray}{0.9}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Constraint & Operation & \iconfluent? \\\hline
\rowcolor{yesgray}
Equality, Inequality & Any & Yes\\
\rowcolor{yesgray}
Uniqueness & Choose some value & Yes\\
Uniqueness & Choose specific value & No\\
\rowcolor{yesgray}
> & Increment & Yes\\
< & Decrement & No \\
\rowcolor{yesgray}
> & Increment & Yes \\
< & Decrement & No \\
\rowcolor{yesgray}
Foreign Key & Insert & Yes\\
\rowcolor{yesgray}
Foreign Key & Delete Cascade & Yes\\
Foreign Key & Delete & No\\
\rowcolor{yesgray}
Secondary Indexing & Update & Yes \\
\rowcolor{yesgray}
Materialized Views & Update & Yes \\
\texttt{AUTO\_INCREMENT} & Insert & No\\\hline
\end{tabular}
\end{center}\vspace{-1em}
\caption{\iconfluence for \lang invariants.}
\label{table:invariants}
\end{table}

With this simple (but expressive) language in mind, we can enumerate a
set of \iconfluent and non-\iconfluent operations. We do not claim
that this enumeration is complete or that \lang is sufficiently
expressive for all applications. However, as we will see
(Section~\ref{sec:evaluation}), it is surprisingly expressive for
many.

Several operations are \iconfluent: any operation on columns without
constraints, increment (decrement) of counters without \textit{less
  than} (\textit{greater than}) constraints, modification of foreign
key columns (provided $i.)$ a suitable remote data column is read as
part of the same transaction or $ii.)$ a suitable remote data column
is modified), and any (in-)equality constraints, \textit{sum\_of}
(with the same caveat as foreign key constraints). Given cluster-wide
unique ID generation (i.e., UUID or via cluster membership), insertion
into primary key columns without autoincrement constraints (provided
the columns are not specified by the end user---e.g., ``give me a new
ID'' versus ``insert this new ID'') are also supported. In contrast,
insertion of specified primary keys, use of autoincrement, and
\textit{less than} and \textit{greater than} for decrement and
increment operations, respectively, are not invariant
commutative. Perhaps not surprisingly, all of these cases can be
checked by simple syntactic rules; we built a simple \lang analysis
tool that identifies all of the above constructs in addition to
limited support for conditional updates in less than a week.

\subsection{Dealing with Conflicts}
\label{sec:conflicts}

If a set of transactions is not \iconfluent, then concurrently
executing combination of the transactions might violate the given
integrity constraint. This requires coordination--but how much?

At one extreme, it is sufficient to perform (serial) mutual exclusion
between any possibly conflicting transactions. For instance, if
procedures $p_1$ and $p_2$ could conflict, then a system could execute
each under a serializable isolation level. This is expensive: for any
\textit{possible} conflicts, transactions will have to coordinate and
potentially block, even for operations in the transactions that do not
conflict.

We suggest an alternative: let transactions execute in isolation and
produce outputs, and then use optimistic concurrency control to check
whether any conflicting outputs were actually produced. This
validation step is possibly expensive, as \textit{every} possible
conflict must be checked. If $\delta_{i1}$ and $\delta_{j1}$ conflict,
any transaction producing either of these outputs must contact a
validator in order to check whether the corresponding conflicting
action has been performed. If so, we have two options. In the basic
case, the transaction must abort due to a would-be conflict of
integrity constraints. We call these updates \textit{abortable
  conflicts}. However, not all conflicts require aborting. For
example, a transaction that enforces that \textit{some} doctor is on
duty need not actually abort if an alternate doctor is actually on
staff. We model these \textit{non-abortable conflicts} as
mini-transactions---effectively, closures that operate atomically on
database state. In the doctor case, the mini-transaction might check
the table for an existing doctor and simply produce no new output. In
a less trivial case, a user might autoincrement an ID counter when
inserting a new row; non-abortable conflicts only require
serialization, not aborts. Of course, this problem of reducing
conflicting operations introduces a new set of challenges.

Fortunately, the problem of minimizing conflicts is well-studied
(Section~\ref{sec:relatedwork}). The most aggressive decomposition
techniques we have encountered are those of Bernstein and Lewis's
``Assertional Concurrency Control'' protocols, which decompose
transactions into minimally-sized atomic units that are subsequently
executed as to avoid pre-condition invalidation (cf. \textit{maximally
  reduced proofs} for non-modular transaction
decomposition~\cite{decomp-semantics}). To do so requires additional
semantics about intermediate pre- and post-conditions for each
operation: this brings this research into the realm of programming
languages. We believe this is a worthwhile area for future work, but,
due to programmer burden, do not further consider these techniques in
this paper.

\subsection{Merge Functions}
\label{sec:merge}

So far, we have assumed a simple bag representation of database
state. This is convenient for proving properties about database state
and reasoning about the behavior of merge functions, but, in practice,
merges may be more complex. For example, for many applications, it is
impractical to retain all versions ever written to the
database. Instead, databases may wish to \textit{compact} these
versions to limit storage requirements. A simple compaction policy is
\textit{last writer wins}, whereby writes are assigned a timestamp and
only the highest-timestamped write to each time is
retained~\cite{dynamo}. However, improper use of last-writer wins can
yield \iconfluent but functionally inconsistent results: consider, for
example, a user that wishes to ensure that no bank account has
negative balance. If using a simple numeric register, if two users
each withdraw \$60 from an account with \$100, a last-writer wins
column might end up with \$40 total. This can be prevented by adding
an additional invariant that states that account balances should
reflect all additions and removals from each account, but this is an
arguably non-trivial pitfall.

We propose the use of abstract data types to avoid anomalies such as
the above. The bank account example is not technically incorrect, but
it reflects an incomplete specification of invariants. In many
real-world examples, users' successful operations should be reflected
in any converged database state; by choosing merge functions that
reflect this ``durability'' requirement, a database can avoid these
odd anomalies. While many practitioners today must hand-code their own
merge functions (e.g., Bayou~\cite{bayou} and Dynamo~\cite{dynamo}
conflict reconciliation), we advocate a model in which a
\textit{library} of type-specific merge functions are presented to
users. This approach---suggested by concurrency control for abstract
data types~\cite{weihl-thesis} and, more recently, Commutative and
Replicated Data Types~\cite{crdt}---simplifies the task of merging
updates while also reducing the overheads of operations. Unlike this
prior work, however, \iconfluence analysis \textit{also} guarantees
the correctness of (specified) arbitrary transformations on these
types, thus eliminating the problem of Conway et al.'s
\textit{scope dilemma}~\cite{blooml}, whereby operations on individual
data items are valid but their composition is not.

\input{isolation-subsec}

\input{confluence-subsec}
