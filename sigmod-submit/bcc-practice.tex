
\section{From Theory to Practice}
\label{sec:bcc-practice}
\label{sec:merge}

Using \iconfluence as test for coordination requirements exposes a
trade-off between the operations a user wishes to perform and the
semantic guarantees she wishes to guarantee about her data. At the
extreme, if a user's transactions do not modify database state, she
can guarantee any invariant that holds over the initial state, while,
with no invariants, a user can safely perform any operations she
likes. While these extremes are trivial, the space in-between contains
a spectrum of interesting and---as we discuss here---useful
combinations to explore. Until now, we have been largely concerned
with formalizing \iconfluence for abstract operations; in this
section, we begin to make these trade-off complete. We examine a
series of practical invariants by exploring increasingly permissive
subsets of SQL, starting from our payroll example and ending with
abstract data types and a discussion of possible limitations and
extensions. We will use these results in our analysis of several
applications in Section~\ref{sec:evaluation}.

\minihead{Revisiting Payroll} We can (briefly) revisit our payroll
example. As we showed by counterexample, uniqueness of arbitrary
values is not \iconfluent: $\{$Stan:5$\}$ and $\{$Mary:5$\}$ are both
valid states that can be reached by valid sequences (starting at
$\{\}$) but their merge---$\{$Stan:5, Mary:5$\}$ is not
valid. Therefore, insertion of arbitrary values is not $I$-confluent
for $I=\{$unique IDs$\}$. However, deletion of employees \textit{is}
$I$-confluent: removing items from a bag of unique values cannot
introduce duplicates. We can repeat the same exercise for foreign key
constraints: inserting new records with valid department IDs cannot in
fact introduce conflicts during merge. The salary constraint is
similarly easily validated for simple insert statements.

\subsection{\iconfluence for Relations}

We begin by considering several constraints found in SQL that are
expressible via standard relational constructs.

\minihead{Equality} Applications often wish to disallow certain values
from records. For example, an application writer might require that an
ID column assume a non-null value by marking the column as \texttt{NOT
  NULL}. These equality (and in-equality) constraints operate on a
per-record basis. We can apply \iconfluence analysis to show that
equality constraints are achievable with \cfreedom. Assume two
database states $S_1$ and $S_2$ each satisfy a per-record equality
invariant $I_e$ but that $I_e(S_1 \sqcup S_2)\rightarrow false$. Then
there exists at least one record $r \in S_1 \sqcup S_2$ such that the
equality constraint fails. Union-based merge is non-destructive and
does not change the value of a given record, so $r \in S_1$ or $r \in
S_2$ (or both). But that would imply that one of $S_1$ or $S_2$
violates $I_e$, a contradiction. Therefore, per-record equality
invariants for arbitrary values are \iconfluent.

For brevity, we omit formal proofs for remaining invariants.

\minihead{Uniqueness} Applications often wish to assert the uniqueness
of values within a given record. For example, an application might
desire that user IDs be unique. If we allow arbitrary insertion and
modification of unique values, then we can easily construct
non-\iconfluent sequences of transactions (e.g., simultaneously
perform \texttt{insert id=0} and \texttt{insert id=0}). Uniqueness is
not \iconfluent for inserts of unique values.

However, if we consider arbitrary \textit{generation} of IDs, whereby
the database generates unique values on behalf of users (e.g., assign
a new user an ID), we can indeed achieve uniqueness---with a notion of
replica membership (e.g., server or replica IDs), deterministically
(e.g., combining a unique replica ID with a sequence number) or with
high probability (e.g., via UUIDs). The difference is subtle (``grant
this record this specific, unique ID'' versus ``grant this record some
unique ID''), but, in a system model with membership or random number
generation (as is pragmatic for most contexts), is powerful. If
replicas only assign IDs that are unique and within their respective
portion of the ID namespace, then merging locally valid states will
also be globally valid.

We can consider further invariants on the unique values: for example,
an \texttt{AUTO\_INCREMENT} constraint dictates that values are
assigned in increasing order. This represents a further refinement to
the above examples. The former (specific value assignment) is still
not \iconfluent, while the latter depends on the semantics of the
constraint. A constraint requiring a dense, unique sequence of IDs
(i.e., no gaps in the ID space) is not \iconfluent. However, as a
consolation, if we can defer the ID assignment until the end of the
transaction (Section~\ref{sec:evaluation}), resolving this
``conflict'' does not necessarily require transaction abort. 

Uniqueness invariants are unsurprisingly \iconfluent under selection
(i.e., read) and deletion.

\minihead{Foreign Keys} Applications often wish to express
relationships between records, captured in SQL by foreign key
constraints. In our payroll example, each employee belongs to a
department.

From the perspective of \iconfluence analysis, foreign key constraints
concern the \textit{visibility} of related updates: if individual
database states maintain referential integrity, a non-destructive
merge function (like our bag union) cannot cause tuples to
``disappear'' and compromise the constraint. Indeed, foreign key
constraints are \iconfluent under insertion and selection. Implicitly,
this may require merging all of a remote replica's data at once (e.g.,
merging the \texttt{employee} table then the \texttt{department} table
might result in inconsistency) but is achievable (even in a
partitioned environment) without compromising availability via
multi-versioning~\cite{ramp-txns,hat-vldb}.

Deletion and modification are more challenging. A na\"{\i}ve
implementation of deletion (i.e., via tombstoning records) might lead
to constraint violation (e.g., an employee is in department that does
not exist in the \texttt{department} table). However, if we only allow
\textit{cascading deletes}, then any ``dangling'' references will also
be deleted on merge, preserving \iconfluence. We can generalize these
concepts to update (and cascading updates).

\minihead{Materialized Views} As a final example within standard SQL,
we can consider the problem of maintaining materialized views. A user
may wish to pre-compute the cardinality of a given relation or query
in order to speed performance via a materialized
view~\cite{gray-book}. When data changes, the materialized view should
change. Merging divergent ``primary'' data should update the
(divergent) materialized views. The updated materialized views can be
computed and installed at the same time as the changes to the primary
data are installed (a problem related to foreign key constraints),
view maintenance is \iconfluent. Synchronously updating the views may
be expensive but is indeed possible in a convergent setting.

\subsection{\iconfluence for Data Types}

Thus far, we have only considered bags of modifications stored in
relations. We can express many useful constraints over these bags, and
the model is a natural fit for, say, immutable
databases~\cite{gray-virtues,gray-book} (which, as the prior section
demonstrated, are not \iconfluent for all invariants). However, in
practice, many database systems do not expose bag semantics, leading
to a variety of interesting anomalies. For example, if we implement a
bank account balance using a ``last writer wins'' merge
policy~\cite{vogels-defs}, then merging the result of two concurrent
withdrawal transactions might result in a database state reflecting
only one transaction (i.e., the Lost Update
phenomenon)~\cite{adya-isolation,hat-vldb}. To support these
anomalies, many database designs have proposed the use of
\textit{abstract data types}, providing merge functions for a variety
of uses such as counters, sets, and
maps~\cite{crdt,atomictransactions,weihl-thesis,blooml} that ensure
that all operations are reflected in converged database state. For
example, a simple op-based CRDT counter might be built from a single
integer that is incremented for each corresponding user-level
\texttt{increment} operation~\cite{crdt}.

\iconfluence is applicable to these data types as well. For example,
it is easy to see that a row-level \texttt{>} threshold invariant is
\iconfluent for \texttt{increment} and \texttt{update} but not
\texttt{decrement}, while a row-level \texttt{<} threshold invariant
is \iconfluent for \texttt{decrement} and \texttt{update} but not
\texttt{increment}. We can similarly guarantee equality but not
in-equality for counters supporting increment and decrement. These
data types (including lists, sets, and maps) can be combined with
standard relational constraints like materialized view maintenance
(e.g., the ``total salary'' row should contain the sum of employee
salaries in the \texttt{employee} table). Importantly, while many
implementations of these data types provide useful properties like
convergence without compromising availability~\cite{blooml,crdt}, they
do \textit{not} guarantee that invariants are not violated. The prior
CRDT counter supporting increment and decrement operations will
guarantee that all operation invocations are reflected in the final
CRDT state but will not guarantee (or enforce) that our given
invariants hold.

\subsection{Discussion and Limitations}

As Table~\ref{table:invariants} summarizes, we have surveyed several
examples of invariants and operations to demonstrate the utility of
(informal) \iconfluence analysis. These examples are by no means
comprehensive, but we have found them to be surprisingly expressive
for many applications (Section~\ref{sec:evaluation}). Moreover, as
many are common to existing SQL dialects, we have found it easy to
automate this process via syntactic, rule-based analysis of
declarative procedures and DDL; a basic prototype identifying
\iconfluence for all of the above constraints in addition to limited
support for conditional updates required less than a week to build.

Through our treatment of abstract data types, we recognize that
immutable/bag semantics are not always desirable for
programmability. In practice, and in our analysis thus far, we have
found abstract data types to be a useful workaround for avoiding
anomalies due to non-bag merge semantics. Indeed, as others have
noted~\cite{bayou,gray-book}, managing update conflicts requires care
to avoid ``losing'' modifications and/or ending up with logically
inconsistent data (e.g., ``return \texttt{NULL}'' is a safe but
trivial merge function). \iconfluence analysis adds application-level
semantics (safety) to the process of merging (liveness) and can be
viewed as determining when it is indeed safe to perform ``optimistic''
replication and merge~\cite{optimistic}. However, unlike ``tentative
update'' programming models, successful (i.e., committed) updates will
not be rolled back.

While our proposed invariants are entirely declarative, a class of
useful semantics---recency guarantees---are not inherently
declarative. Users often wish to read data that is up-to-date as of a
given point in time (e.g., ``read latest''~\cite{pnuts}). While
serializability and traditional isolation models do not address these
recency guarantees~\cite{adya-isolation}, they are indeed important to
programmers. We can simulate recency requirements in \iconfluence
analysis by logging the result of all reads with a timestamp and
requiring that the logged timestamps obey their recency guarantees,
but it is known that recency guarantees are unachievable with
transactional availability~\cite{hat-vldb}. If users wish to ``read
their writes'' (i.e., ``session'' guarantees~\cite{bayou}), they can
do so by maintaining affinity or
``stickiness''~\cite{hat-vldb,vogels-defs} with a given set of
replicas, while ``bounded staleness'' guarantees for reads are
achievable with multi-versioning or read
replicas~\cite{pnuts}. Otherwise, linearizable
semantics~\cite{spanner} will require coordination. Indeed, when
``consistency'' means ``recency,'' we cannot circumvent speed-of-light
delays.

% understanding the limits -- merges, recency, escrow, immutability

\begin{table}
\definecolor{yesgray}{gray}{0.95}
\begin{center}
\small
\begin{tabular}{|l|l|c|}
\hline
\textbf{Invariant} & \textbf{Operation} & \textbf{\iconfluent?} \\\hline

\rowcolor{yesgray}
Equality & Any & Yes\\
Inequality & Any & Yes \\
Uniqueness & Choose specific value & No\\
\rowcolor{yesgray}
Uniqueness & Choose some value & Yes\\
\texttt{AUTO\_INCREMENT} & Insert & No\\
\rowcolor{yesgray}
Foreign Key & Insert & Yes\\
Foreign Key & Delete & No\\
\rowcolor{yesgray}
Foreign Key & Cascading Delete & Yes\\
\rowcolor{yesgray}
Secondary Indexing & Update & Yes \\
\rowcolor{yesgray}
Materialized Views & Update & Yes \\\hline\hline
\rowcolor{yesgray}
> & Increment [Counter] & Yes\\
< & Decrement [Counter] & No \\
\rowcolor{yesgray}
> & Increment [Counter] & Yes \\
< & Decrement [Counter] & No \\
\rowcolor{yesgray}
\texttt{[NOT] CONTAINS} & Any [Set, List, Map] & Yes \\ 
\texttt{HEAD=,TAIL=,length=} & Mutation [List] & No \\ \hline
\end{tabular}
\end{center}\vspace{-1em}
\caption{Example SQL (top) and ADT invariant \iconfluence.}
\label{table:invariants}
\end{table}
